# 统计学习方法 （李航）

[toc]

## 第六章 逻辑斯谛回归与最大熵模型

逻辑斯谛回归与最大熵模型都属于对数线性模型。

### 6.1 逻辑斯谛回归模型

#### 6.1.1 逻辑斯谛分布

设X时连续随机变量，X服从逻辑斯谛分布，具有以下分布函数和密度函数
$$
F(x)=P(X\le x)=\frac{1}{1+e^{-(x-\mu)/\lambda}}\\
f(x)=F’(x)=\frac{e^{-(x-\mu)/\lambda}}{\lambda(1+e^{-(x-\mu)/\lambda})^2}
$$
式中，$\mu$为位置参数，$\lambda >0$为形状参数。

#### 6.1.2 二项逻辑斯谛回归

该模型是如下的条件概率分布：
$$
P(Y=1|x)=\frac{\exp(w\cdot x+b)}{1+\exp(w\cdot x+b)}\\
P(Y=0|x)=\frac{1}{1+\exp(w\cdot x+b)}
$$
$w$为权值，$b$为偏置

一个时间的几率，odds是指该事件发生的概率与该事件不发生的概率的比值。称事件的对数几率，log odds或logit函数为
$$
\text{logit}(p)=\log\frac{p}{1-p}
$$
对逻辑斯谛回归而言，结合（2）（3）式得
$$
\log\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\cdot x + b
$$
这意味着**输出$Y=1$的对数几率式输入x的线性函数**

#### 6.1.3 参数估计

设
$$
P(Y=1|x)=\mathbf{\pi}(x)，\mbox{     }P(Y=0|x)=1-\mathbf{\pi}(x)
$$
则似然函数为
$$
\prod_{i=1}^N[\pi(x_i)]^y_i[1-\pi(x_i)]^{1-y_i}
$$
而对数似然函数为
$$
\begin{align*}
L(w)&=\sum_{i=1}^N[y_i\log\pi(x_i)+(1-y_i)\log(1-\pi(x_i))]\\
&=\sum_{i=1}^Ny_i\log\frac{\pi(x_i)}{1-\pi(x_i)}+\log(1-\pi(x_i))\\
&=\sum_{i=1}^Ny_i(w\cdot x_i)-\log(1+\exp(w\cdot x_i))
\end{align*}
$$
逻辑斯谛回归嘘唏通常采用梯度下降及拟牛顿法。

假设w的极大似然估计是$\hat w$，那么学到的模型为
$$
P(Y=1|x)=\frac{\exp(\hat w\cdot x+b)}{1+\exp(\hat w\cdot x+b)}\\
P(Y=0|x)=\frac{1}{1+\exp(\hat w\cdot x+b)}
$$
多项逻辑斯谛回归模型是
$$
P(Y=k|x)=\frac{\exp(w_k\cdot x)}{1+\sum_{k=1}^{K-1}\exp(w_k\cdot x)},\: k=1,2···,K
$$

$$
P(Y=K|x)=\frac{1}{1+\sum_{k=1}^{K-1}\exp(w_k\cdot x)}
$$

### 6.2 最大熵模型

#### 6.2.1 最大熵原理

最大熵原理是概率模型学习的一个准则，最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型时最好的模型。**通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型中选取熵最大的模型。**

