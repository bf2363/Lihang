# 统计学习方法 （李航）

[toc]

## 第六章 逻辑斯谛回归与最大熵模型

逻辑斯谛回归与最大熵模型都属于对数线性模型。

### 6.1 逻辑斯谛回归模型

#### 6.1.1 逻辑斯谛分布

设X时连续随机变量，X服从逻辑斯谛分布，具有以下分布函数和密度函数
$$
F(x)=P(X\le x)=\frac{1}{1+e^{-(x-\mu)/\lambda}}\\
f(x)=F’(x)=\frac{e^{-(x-\mu)/\lambda}}{\lambda(1+e^{-(x-\mu)/\lambda})^2}
$$
式中，$\mu$为位置参数，$\lambda >0$为形状参数。

#### 6.1.2 二项逻辑斯谛回归

该模型是如下的条件概率分布：
$$
P(Y=1|x)=\frac{\exp(w\cdot x+b)}{1+\exp(w\cdot x+b)}\\
P(Y=0|x)=\frac{1}{1+\exp(w\cdot x+b)}
$$
$w$为权值，$b$为偏置

一个事件的几率，odds是指该事件发生的概率与该事件不发生的概率的比值。称事件的对数几率，log odds或logit函数为
$$
\text{logit}(p)=\log\frac{p}{1-p}
$$
对逻辑斯谛回归而言，结合（2）（3）式得
$$
\log\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\cdot x + b
$$
这意味着**输出$Y=1$的对数几率式输入x的线性函数**

#### 6.1.3 参数估计

设
$$
P(Y=1|x)=\mathbf{\pi}(x)，\mbox{     }P(Y=0|x)=1-\mathbf{\pi}(x)
$$
则似然函数为
$$
\prod_{i=1}^N[\pi(x_i)]^y_i[1-\pi(x_i)]^{1-y_i}
$$
而对数似然函数为
$$
\begin{align*}
L(w)&=\sum_{i=1}^N[y_i\log\pi(x_i)+(1-y_i)\log(1-\pi(x_i))]\\
&=\sum_{i=1}^Ny_i\log\frac{\pi(x_i)}{1-\pi(x_i)}+\log(1-\pi(x_i))\\
&=\sum_{i=1}^Ny_i(w\cdot x_i)-\log(1+\exp(w\cdot x_i))
\end{align*}
$$
逻辑斯谛回归学习通常采用**梯度下降及拟牛顿法。**

假设w的极大似然估计是$\hat w$，那么学到的模型为
$$
P(Y=1|x)=\frac{\exp(\hat w\cdot x+b)}{1+\exp(\hat w\cdot x+b)}\\
P(Y=0|x)=\frac{1}{1+\exp(\hat w\cdot x+b)}
$$
多项逻辑斯谛回归模型是
$$
P(Y=k|x)=\frac{\exp(w_k\cdot x)}{1+\sum_{k=1}^{K-1}\exp(w_k\cdot x)},\: k=1,2···,K
$$

$$
P(Y=K|x)=\frac{1}{1+\sum_{k=1}^{K-1}\exp(w_k\cdot x)}
$$

### 6.2 最大熵模型

#### 6.2.1 最大熵原理

最大熵原理是概率模型学习的一个准则，最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型时最好的模型。**通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型中选取熵最大的模型。**

#### 6.2.2 最大熵模型的定义

假设分类模型是一个条件概率分布$P(Y|X),X\in\mathcal{X}\subseteq\mathbf{R}^n$表示输入，$Y\in\mathcal{Y}$表示输出，X，Y分别时输入和输出的集合。这个模型表示的是对给定的输入，以条件概率$P(Y|X)$给出输出。

**首先考虑模型应该满足的条件。**给定训练数据集，可以确定联合分布$P(X,Y)$的经验分布，和边缘分布$P(X)$的经验分布。表示如下
$$
\tilde{P}(X=x,Y=y)=\frac{v(X=x,Y=y)}{N}
$$

$$
\tilde{P}(X=x)=\frac{v(X=x)}{N}
$$

易得$\sum_y\tilde{P}(X=x,Y=y)=\tilde{P}(X=x)$

其中，$v(X=x,Y=y)$表示训练数据中样本（x,y)出现的频数。

用特征函数，feature function$f(x,y)$描述输入x和y之间的某一个事实，定义为
$$
f(x,y)=\begin{cases}
1, & x与y满足某一事实\\
0, & 否则
\end{cases}
$$
特征函数关于经验分布$\tilde{P}(X,Y)$的期望值，可表示为
$$
E_{\tilde{P}}(f)=\sum_{x,y}\tilde{P}(x,y)f(x,y)
$$
特征函数关于模型$P(Y|X)$与经验分布$\tilde{P}(X)$的期望值，可表示为
$$
E_{P}(f)=\sum_{x,y}\tilde{P}(x)P(y|x)f(x,y)
$$
如果模型能够获取训练数据的信息，那么可以假设这两个期望值相等
$$
E_{\tilde{P}} = E_{P}
$$

$$
\sum_{x,y}\tilde{P}(x,y)f(x,y)=\sum_{x,y}\tilde{P}(x)P(y|x)f(x,y)
$$

我们将上式作为模型学习的约束条件，假设有n个特征函数，那么就有n个约束条件

假设满足所有约束条件的模型集合为
$$
\mathcal{C}\equiv\{P\in\mathcal{P}|E_P(f_i)=E_{\tilde{P}}(f_i),i=1,2,···,n\}
$$
定义在条件概率分布$P(Y|X)$熵的条件熵为
$$
H(P)=-\sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)
$$
则在集合中条件熵最大的模型称为最大熵模型。

#### 6.2.3 最大熵模型的学习

对于给定的训练数据集$T=\{(x_1,y_1),(x_2,y_2),···,(x_N,y_N)\}$以及特征函数$f_i(x,y)$，最大熵模型的学习等价于约束最优化问题
$$
\max_{P\in\mathbf{C}}\; H(P)=-\sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)\\
s.t. \; E_P(f_i)=E_{\hat{P}}(f_i)\\
\sum_yP(y|x)=1
$$
这里，将约束最优化的原始问题转换为无约束最优化的对偶问题，通过引入拉格朗日乘子，定义拉格朗日函数求解。
$$
\begin{align*}
L(P,w)&\equiv-H(P)+w_0(1-\sum_yP(y|x))+\sum_{i=1}^nw_i(E_{\tilde{P}}(f_i)-E_P(f_i))\\
&=\sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)+w_0(1-\sum_yP(y|x))\\
&+\sum_{i=1}^nw_i(\sum_{x,y}\tilde{P}(x,y)f_i(x,y)-\sum_{x,y}\tilde{P}(x)P(y|x)f_i(x,y))
\end{align*}
$$
最优化的原始问题是
$$
\min_{P\in\mathbf{C}}\max_wL(P,w)
$$
对偶问题是
$$
\max_w\min_{P\in\mathbf{C}}L(P,w)
$$
**由于拉格朗日函数是P的凸函数，原始问题的解与对偶问题的解是等价的。**

首先，求解对偶问题内部的极小化问题。将其记作
$$
\Psi(w)=\min_{P\in\mathbf{C}}L(P,w)=L(P_w,w)
$$
将其解记作
$$
P_w=\arg\min_{P\in\mathbf{C}}L(P,w)=P_w(y|x)
$$
具体地，求解$L(P,w)$对$P(y|x)$的偏导数
$$
\begin{align*}
\frac{\partial L(P,w)}{\partial P(y|x)}&=\sum_{x,y}\tilde{P}(x)(\log P(y|x)+1)-\sum_yw_0-\sum_{x,y}(\tilde{P}(x)\sum_{i=1}^nw_if_i(x,y))\\
&=\sum_{x,y}\tilde{P}(x)(\log P(y|x)+1-w_0-\sum_{i=1}^nw_if_i(x,y))
\end{align*}
$$
令偏导数等于0，且在$\tilde{P}(x)>0$的情况下，解得
$$
P(y|x)=\exp(w_0+\sum_{i=1}^nw_if_i(x,y)-1)=\frac{\exp(\sum_{i=1}^nw_if_i(x,y))}{\exp(1-w_0)}
$$
由于$\sum_yP(y|x)=1$得
$$
P_w(y|x)=\frac{1}{Z_w(x)}\exp(\sum_{i=1}^nw_if_i(x,y))\\
Z_w(x)=\sum_y\exp(\sum_{i=1}^nw_if_i(x,y))
$$
这里，$Z_w(x)$称为规范化因子，wi是特征函数的权值。之后，求解对偶问题外部的极大化问题
$$
\max_w\Psi(w)
$$

#### 6.2.4 极大似然估计

**最大熵模型学习中的对偶函数极大化等价于最大熵模型的极大似然估计**

已知训练数据的经验概率分布，条件概率分布的对数似然函数表示为
$$
\begin{align*}
L_{\tilde{P}}(P_w)&=\log\prod_{x,y}P(y|x)^{\tilde{P}(x,y)}\\
&=\sum_{x,y}\tilde{P}(x,y)\log P(y|x)\\
&=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_{x,y}\tilde{P}(x,y)\log Z_w(x)\\
&=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_{x}\tilde{P}(x)\log Z_w(x)
\end{align*}
$$
再看对偶函数，由上式可得
$$
\begin{align*}
\Psi(w)&=L(P_w,w)\\
&=\sum_{x,y}\tilde{P}(x)P_w(y|x)\log P_w(y|x)+w_0(1-\sum_yP_w(y|x))\\
&+\sum_{i=1}^nw_i(\sum_{x,y}\tilde{P}(x,y)f_i(x,y)-\sum_{x,y}\tilde{P}(x)P_w(y|x)f_i(x,y))\\
&=\sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)+0+\sum_{i=1}^nw_i(\sum_{x,y}\tilde{P}(x,y)f_i(x,y)-\sum_{x,y}\tilde{P}(x)P(y|x)f_i(x,y))\\
&=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)+\sum_{x,y}\tilde{P}(x)P_w(y|x)(\log P_w(y|x)-\sum_{i=1}^nw_if_i(x,y))\\
&=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_{x,y}\tilde{P}(x)P_w(y|x)\log Z_w(x)\\
&=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_{x}\tilde{P}(x)\log Z_w(x)
\end{align*}
$$
最后一步用到了$\sum_yP(y|x)=1$,从而可得对偶函数等价于对数似然函数。

### 6.3 模型学习的最优化算法

逻辑斯谛回归模型、最大熵模型学习归结为以**似然函数为目标函数的最优化问题，通常以迭代算法求解。**从最优化的观点看，这时的目标函数具有很好的性质。它是光滑的凸函数，因此多种最优化的方法都适用，保证能找到全局最优解。

常用的方法有改进的迭代尺度法，梯度下降法，牛顿法或拟牛顿法。**牛顿法或拟牛顿法一般收敛速度更快。**

#### 6.3.1 改进的迭代尺度法

改进的迭代尺度法，improved iterative scaling，IIS。

已知最大熵模型为
$$
P_w(y|x)=\frac{1}{Z_w(x)}\exp(\sum_{i=1}^nw_if_i(x,y))\\
Z_w(x)=\sum_y\exp(\sum_{i=1}^nw_if_i(x,y))
$$
对数似然函数为
$$
L(w)=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_x\tilde{P}(x)\log Z_w(x)
$$
IIS的想法是，假设最大熵模型当前的参数向量是$w=(w_1,w_2,···,w_n)^T$，我们希望找到一个新的向量$w+\sigma=(w_1+\sigma_1,w_2+\sigma_2,···,w_n+\sigma_n)^T$，使得模型的对数似然函数值增大。如果能有这种该参数更新方法，则更新参数，重复使用这一方法，直至找到对数似然函数的最大值。

对于给定的经验分布，模型的参数更新后，对数似然函数的该变量是
$$
\begin{align*}
L(w+\sigma)-L(w)&=\sum_{x,y}\tilde{P}(x,y)\log P_{w+\sigma}(y|x)-\sum_{x,y}\tilde{P}(x,y)\log P_{w}(y|x)\\
&=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\sigma_if_i(x,y)-\sum_x\tilde{P}(x)\log\frac{Z_{w+\sigma}(x)}{Z_{w}(x)}
\end{align*}
$$
利用不等式$-\log a\ge1-a,a>0$，则可以建立对数似然函数变量的下界
$$
\begin{align*}
L(w+\sigma)-L(w)&\ge\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\sigma_if_i(x,y)+1-\sum_x\tilde{P}(x)\frac{Z_{w+\sigma}(x)}{Z_{w}(x)}\\
&=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\sigma_if_i(x,y)+1-\sum_x\tilde{P}(x)\sum_yP_w(y|x)\exp\sum_{i=1}^n\sigma_if_i(x,y)
\end{align*}
$$
记
$$
A(\sigma|w)=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\sigma_if_i(x,y)+1-\sum_x\tilde{P}(x)\sum_yP_w(y|x)\exp\sum_{i=1}^n\sigma_if_i(x,y)
$$
于是上式为对数似然函数改变量的一个下界。

如果能够找到适当的$\sigma$使下界提高，那么对数似然函数也会提高。然而$\sigma$是一个向量，含有多个变量。IIS试图以此只优化其中一个变量，$\sigma_i$，固定其他变量。  