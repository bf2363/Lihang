# 统计学习方法 （李航）

[toc]

## 第四章 朴素贝叶斯

### 4.1 朴素贝叶斯的学习与分类

#### 4.1.1 基本方法

$P(X,Y)$是X和Y的联合概率分布，训练数据集
$$
T=\{(x_1,y_1),(x_2,y_2),···(x_N,y_N)\}
$$
由$P(X,Y)$**独立同分布产生**

朴素贝叶斯学习**先验概率分布以及条件概率分布**。先验概率分布
$$
P(Y=c_k),k=1,2,···,K
$$
条件概率分布
$$
P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},···,X^{(n)}=x^{(n)}|Y=c_k),k=1,2,···,K
$$
从而学习到联合概率分布。

条件概率分布$P(X=x|Y=c_k)$有指数级数量的参数，其估计实际是不可行的。事实上，假设$x^{(j)}$可取值有$S_j$个，$j=1,2,···，n$，Y可取值K个，那么参数个数为$K\prod_{j=1}^nS_j$个。

朴素贝叶斯对条件概率分布做了**条件独立性假设**。由于这是一个较强的假设，朴素贝叶斯由此得名。具体的，条件独立性假设是
$$
\begin{align*}
P(X=x|Y=c_k)&=P(X^{(1)}=x^{(1)},···,X^{(n)}=x^{(n)}|Y=c_k)\\
&=\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
\end{align*}
$$
条件独立假设等于说是用于分类的特征在类确定的条件下都是条件独立的。

朴素贝叶斯分类时，对给定的输入x，通过学习到的模型计算后验概率分布，将**后验概率最大的类作为x的类输出**。厚颜概率的计算根据贝叶斯定理进行：
$$
\begin{align*}
P(Y=c_k|X=x)&=\frac{P(X=x|Y=c_k)\cdot P(Y=c_k)}{P(X=x,Y=c_k)}\\
&=\frac{P(X=x|Y=c_k)\cdot P(Y=c_k)}{\sum_{k}P(X=x|Y=c_k)\cdot P(Y=c_k)}
\end{align*}
$$
将条件独立性假设带入有
$$
P(Y=c_k|X=x)=\frac{\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)\cdot P(Y=c_k)}{\sum_{k}\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)\cdot P(Y=c_k)},k=1,2,···,K
$$
于是朴素贝叶斯分类器可以表示为
$$
y=f(x)=\arg\max_{c_k} \frac{\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)\cdot P(Y=c_k)}{\sum_{k}\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)\cdot P(Y=c_k)}
$$
注意到分母对所有$c_k$都是相同的吗，所以可简化为
$$
y=f(x)=\arg\max_{c_k}\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)\cdot P(Y=c_k)
$$

#### 4.1.2 后验概率最大化的含义

朴素贝叶斯将实例分到后验概率最大的类中，这**等价于期望风险最小化**。下证明：

假设选择0-1损失函数：
$$
L(Y,f(x))=\begin{cases}
1, & Y\ne f(x)\\
0, & Y=f(x)
\end{cases}
$$
这时期望风险函数为
$$
R_{exp}(f)=E[L(Y,f(X))]=E_X\sum_{k=1}^K[L(c_k,f(X))]P(c_k|X)
$$
为使得期望分线最小化，只需对$X=x$逐个极小化，由此得到：
$$
\begin{align*}
f(x)&=\arg\min_{y\in\mathcal{Y}}\sum_{k=1}^KL(c_k,y)P(c_k|X=x)\\
&=\arg\min_{y\in\mathcal{Y}}\sum_{k=1}^KP(y\neq c_k|X=x)\\
&=\arg\min_{y\in\mathcal{Y}}(1 - P(y=c_k|X=x))\\
&=\arg\max_{y\in\mathcal{Y}}P(y=c_k|X=x)\\
\end{align*}
$$
即为式（5）