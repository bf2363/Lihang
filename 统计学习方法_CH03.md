# 统计学习方法 （李航）

[toc]

## 第三章 k近邻法

### 3.1 算法

1）根据给定的距离度量，在训练集中找出与x最邻近的k个点，涵盖这k个点的x的邻域记作$N_x(x)$；

2）在$N_x(x)$中根据分类决策规则，如多数表决，决定x的类别：
$$
y=\arg\max_{c_j}\sum_{x_i\in N_k(x)}I(y_i=c_j),i=1,2,···,N;j=1,2,···,K
$$
其中$I$为指示函数。k近邻的特殊情是k=1的情形，称为最近邻算法，对输入的实例点x，最近邻将训练集中与x最近邻的类作为x的类。

### 3.2 模型

#### 3.2.2 距离度量

设特征空间，$\mathcal{X}$是n维实数向量空间$\mathbf{R}^n$，$x_i,x_j\in \mathcal{X},x_i=(x_i^{(1)},x_i^{(2)},···，x_i^{(n)})^T,x_j=(x_j^{(1)},x_j^{(2)},···，x_j^{(n)})^T,x_i,x_j$的距离$L_p$定义为
$$
L_p(x_i,x_j) = (\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}},p\ge1
$$
当p=2时为欧式距离，Euclidean distance;

当p=1时为曼哈顿距离，Manhattan distance；

当$p=\infty$时，它是各个坐标距离的最大值，即
$$
L_{\infty}(x_i,x_j)=\max_i|x_i^{(l)}-x_j^{(l)}|
$$

#### 3.2.4 分类决策规则

k近邻法中的**分类决策规则往往是多数表决**，即由输入实例的k个邻近的训练实例中的多数类决定输入实列的类。

多数表决有如下解释，分类函数为
$$
f:\mathbf{R}^n\rightarrow\{c_1,c_2,···,c_K\}
$$
那么误分类的概率为
$$
P(Y\neq f(X))=1-P(Y=f(X))
$$
对给定的实列$x\in\mathcal{X}$，其近邻的k个训练实例点构成集合$N_k(x)$.如果涵盖$N_k(x)$的区域的类别是$c_j$，那么误分类率是
$$
\frac{\sum_{x_i\in N_k(x)}I(y_i\neq c_j)}{k}=1 - \frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i= c_j)
$$
要是误分类最小，即使$\sum_{x_i\in N_k(x)}I(y_i= c_j)$最大。**这同多数表决的形式一致，即多数表决规则等价于经验风险最小化。**

### 3.3 实现：kd树

