# 统计学习方法 （李航）

[toc]

## 第三章 k近邻法

### 3.1 算法

1）根据给定的距离度量，在训练集中找出与x最邻近的k个点，涵盖这k个点的x的邻域记作$N_x(x)$；

2）在$N_x(x)$中根据分类决策规则，如多数表决，决定x的类别：
$$
y=\arg\max_{c_j}\sum_{x_i\in N_k(x)}I(y_i=c_j),i=1,2,···,N;j=1,2,···,K
$$
其中$I$为指示函数。k近邻的特殊情是k=1的情形，称为最近邻算法，对输入的实例点x，最近邻将训练集中与x最近邻的类作为x的类。

### 3.2 模型

#### 3.2.2 距离度量

设特征空间，$\mathcal{X}$是n维实数向量空间$\mathbf{R}^n$，$x_i,x_j\in \mathcal{X},x_i=(x_i^{(1)},x_i^{(2)},···，x_i^{(n)})^T,x_j=(x_j^{(1)},x_j^{(2)},···，x_j^{(n)})^T,x_i,x_j$的距离$L_p$定义为
$$
L_p(x_i,x_j) = (\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}},p\ge1
$$
当p=2时为欧式距离，Euclidean distance;

当p=1时为曼哈顿距离，Manhattan distance；

当$p=\infty$时，它是各个坐标距离的最大值，即
$$
L_{\infty}(x_i,x_j)=\max_i|x_i^{(l)}-x_j^{(l)}|
$$

#### 3.2.4 分类决策规则

k近邻法中的**分类决策规则往往是多数表决**，即由输入实例的k个邻近的训练实例中的多数类决定输入实列的类。

多数表决有如下解释，分类函数为
$$
f:\mathbf{R}^n\rightarrow\{c_1,c_2,···,c_K\}
$$
那么误分类的概率为
$$
P(Y\neq f(X))=1-P(Y=f(X))
$$
对给定的实列$x\in\mathcal{X}$，其近邻的k个训练实例点构成集合$N_k(x)$.如果涵盖$N_k(x)$的区域的类别是$c_j$，那么误分类率是
$$
\frac{\sum_{x_i\in N_k(x)}I(y_i\neq c_j)}{k}=1 - \frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i= c_j)
$$
要是误分类最小，即使$\sum_{x_i\in N_k(x)}I(y_i= c_j)$最大。**这同多数表决的形式一致，即多数表决规则等价于经验风险最小化。**

### 3.3 实现：kd树

kd树是**二叉树**，表示对k维空间的划分，partition。

构造kd树，相当于不断用垂直于坐标轴的超平面将k为空间划分，构成一些列的k维超矩形区域。

kd树的每个结点，对应于一个k维超矩形区域。算法如下

输入：k维空间数据集
$$
T=\{x_1,x_2,···，x_N\},\\
x_i=(x_i^{(1)},x_i^{(2)},···，x_i^{(k)})^T,i=1,2,···,N
$$
1）构造根结点，根结点对应于，包含T的k维空间超矩形区域。

选择$x^{(1)}$为坐标轴，以T中所有实例的$x^{(1)}$坐标的**中位数**为切分点，将根结点对那个的超矩形区域划分为两个子区域。切分由通过切分点，并与坐标轴$x^{(1)}$垂直的超平面实现。

由根结点生成深度为1的左、右子结点，左子结点对应坐标$x^{(1)}$小于切分点；右对应大于。**将落在切分超平面上的实例点保存在根结点**。

2）重复：对深度为j的结点，选择$x^{(l)}$为切分的坐标轴，$l=j(\mod k)+1$，以该结点的区域中所有实例的$x^{(l)}$坐标的中位数为切分点进行切分。生成深度为j+1的左右子结点。

3）直到两个子区域没有实例存在时停止。从而形成kd树的划分。

#### 3.3.2 搜索kd树

给定一个目标点，搜索其最近邻。首先找到包含目标点的叶结点；然后从该叶结点出发，依次回退到父结点；不断查找与目标点最邻近的结点，当确定不可能存在更近的结点时终止。这样搜索就被限制在空间的局部区域上，效率大为提高。

算法

输入：已构造的kd树；目标点x；

输出：x的最近邻，

（1）**在kd树中找出包含目标点x的叶结点**：从根结点出发，递归向下访问kd树。若目标点x当前维的坐标小于切分点的坐标，则移动到左子结点，否则到右子结点。直到子结点为叶结点为止。

（2）以此叶结点为“当前最近点”。

（3）递归地向上回退，在每个结点进行以下操作：

​		（a）如果该结点保存的实例点比当前最近点距离更近，则以该实例点为当前最近点

​		（b）“当前最近点”一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一个子结点对应的区域是否由更近的点。具体地，检查另一个结点对应的区域是否与以目标点为球心、以目标点与当前最近点间的距离的超球体相交。如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，则**移动到另一个子结点**。如果不相交，向上回退。

（4）当回退到根节点时，搜索结束。最后的当前最近点即为x的最近点。

kd树的平均计算复杂度时$O(\log N)$。kd树适用于训练实例数远大于空间维数的搜索。**当空间维数接近训练实例数时，效率会迅速下降，接近线性扫描。**

 

