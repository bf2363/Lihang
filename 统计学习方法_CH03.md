# 统计学习方法 （李航）

[toc]

## 第三章 k近邻法

### 3.1 算法

1）根据给定的距离度量，在训练集中找出与x最邻近的k个点，涵盖这k个点的x的邻域记作$N_x(x)$；

2）在$N_x(x)$中根据分类决策规则，如多数表决，决定x的类别：
$$
y=\arg\max_{c_j}\sum_{x_i\in N_k(x)}I(y_i=c_j),i=1,2,···,N;j=1,2,···,K
$$
其中$I$为指示函数。k近邻的特殊情是k=1的情形，称为最近邻算法，对输入的实例点x，最近邻将训练集中与x最近邻的类作为x的类。

### 3.2 模型

#### 3.2.2 距离度量

设特征空间，$\mathcal{X}$是n维实数向量空间$\mathbf{R}^n$，$x_i,x_j\in \mathcal{X},x_i=(x_i^{(1)},x_i^{(2)},···，x_i^{(n)})^T,x_j=(x_j^{(1)},x_j^{(2)},···，x_j^{(n)})^T,x_i,x_j$的距离$L_p$定义为
$$
L_p(x_i,x_j) = (\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}},p\ge1
$$
当p=2时为欧式距离，Euclidean distance;

当p=1时为曼哈顿距离，Manhattan distance；

当$p=\infty$时，它是各个坐标距离的最大值，即
$$
L_{\infty}(x_i,x_j)=\max_i|x_i^{(l)}-x_j^{(l)}|
$$

#### 3.2.4 分类决策规则

k近邻法中的**分类决策规则往往是多数表决**，即由输入实例的k个邻近的训练实例中的多数类决定输入实列的类。

多数表决有如下解释，分类函数为
$$
f:\mathbf{R}^n\rightarrow\{c_1,c_2,···,c_K\}
$$
那么误分类的概率为
$$
P(Y\neq f(X))=1-P(Y=f(X))
$$
对给定的实列$x\in\mathcal{X}$，其近邻的k个训练实例点构成集合$N_k(x)$.如果涵盖$N_k(x)$的区域的类别是$c_j$，那么误分类率是
$$
\frac{\sum_{x_i\in N_k(x)}I(y_i\neq c_j)}{k}=1 - \frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i= c_j)
$$
要是误分类最小，即使$\sum_{x_i\in N_k(x)}I(y_i= c_j)$最大。**这同多数表决的形式一致，即多数表决规则等价于经验风险最小化。**

### 3.3 实现：kd树

kd树是**二叉树**，表示对k维空间的划分，partition。

构造kd树，相当于不断用垂直于坐标轴的超平面将k为空间划分，构成一些列的k维超矩形区域。

kd树的每个结点，对应于一个k维超矩形区域。算法如下

输入：k维空间数据集
$$
T=\{x_1,x_2,···，x_N\},\\
x_i=(x_i^{(1)},x_i^{(2)},···，x_i^{(k)})^T,i=1,2,···,N
$$
1）构造根结点，根结点对应于，包含T的k维空间超矩形区域。

选择$x^{(1)}$为坐标轴，以T中所有实例的$x^{(1)}$坐标的**中位数**为切分点，将根结点对那个的超矩形区域划分为两个子区域。切分由通过切分点，并与坐标轴$x^{(1)}$垂直的超平面实现。

由根结点生成深度为1的左、右子结点，左子结点对应坐标$x^{(1)}$小于切分点；右对应大于。**将落在切分超平面上的实例点保存在根结点**。

2）重复：对深度为j的结点，选择$x^{(l)}$为切分的坐标轴，$l=j(\mod k)+1$，以该结点的区域中所有实例的$x^{(l)}$坐标的中位数为切分点进行切分。生成深度为j+1的左右子结点。

3）直到两个子区域没有实例存在时停止。从而形成kd树的划分。

