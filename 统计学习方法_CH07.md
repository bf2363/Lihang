# 统计学习方法 （李航）

[toc]

## 第七章 支持向量机

### 7.1 线性可分支持向量机与硬间隔最大化

支持向量机的学习是在特征空间进行的。

#### 7.1.1 线性可分支持向量机

考虑一个二分问题，学习的目的是在特征空间找到一个分离超平面，能将实例分到不同过的类。分离超平面对应于方程$w\cdot x+b=0$，它由法向量w和截距b决定。

一般的，当训练数据集线性可分是，存在无穷个分离超平面，类比于上下确界。感知机利用误分类最小的策略，求得分离超平面，从而解有无穷多个。线性可分支持向量机利用间隔最大化求最有分离超平面，这时，**解释唯一的**。

定义

线性可分支持向量机，给定线性可分训练数据集，通过间隔最大化，或等价地求解相应的凸二次规划问题学习得到的分离超平面为
$$
w^{*}\cdot x+b^*=0
$$
相应的分类决策函数为
$$
f(x)=sign(w^{*}\cdot x+b^*)
$$
称为线性可分支持向量机

#### 7.1.2 函数间隔和几何间隔

一般来说，**一个点距离分离超平面的远近，可以表示分类预测的确信程度。**在超平面确定的情况下，$|w\cdot x+b|$能相对地表示点距离超平面的远近，而$w\cdot x+b$的符号于类标记y的符号是否一致，表示分类是否正确。

所以$y(w\cdot x+b)$表示分类的正确性以及确信度。这就是函数间隔的概念，functional margin。

但是只要成比例的改变w和b，例如将它们改为2w和2b，超平面并没有改变，但是函数间隔变为了原来的2倍。

所以对分离超平面的法向量加以约束，如规范化，$||w||=1$，使得间隔是确定的。这时函数间隔变为几何间隔，geometric margin。
$$
\gamma_i=y_i(\frac{w}{||w||}\cdot x_i+\frac{b}{||w||})
$$

#### 7.1.3 间隔最大化

间隔最大化的直观解释是，对训练数据集找到集合间隔最大的超平面意味着以充分大的确信度，对训练数据进行分类。这里的间隔最大化又称为硬间隔最大化。

考虑下面约束优化问题
$$
\max_{w,b}\;\gamma\\
s.t.\; y_i(\frac{w}{||w||}\cdot x_i+\frac{b}{||w||})\ge\gamma,\;i=1,2,···,N
$$
通过设定$\gamma\ge0$来使得分类正确
$$
\max_{w,b}\;\frac{\hat{\gamma}}{||w||}\\
s.t.\; y_i(w\cdot x_i+b)\ge\hat\gamma,\;i=1,2,···,N
$$
其中$\hat\gamma=\min_iy_i(w\cdot x_i+b)$,是函数间隔。**由于函数间隔的大小并不影响最优化问题的解，事实上，假设将w和b按比例改变为$\lambda w,\;\lambda b$，这时函数间隔称为$\lambda\hat\gamma$。**函数间隔的这一改变，对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响。即，产生一个等价的最优化问题。

这样，我们取$\hat\gamma=1$，将其带入上述的最优化问题。同时又注意到，最大化$\frac{1}{||w||}$同最小化$\frac{||w||^2}{2}$等价，从而有以下最优化问题
$$
\min_{w,b}\frac{||w||^2}{2}\\
s.t.\; y_i(w\cdot x_i+b)-1\ge0,\;i=1,2,···,N
$$

这是一个凸二次规划，convex quadratic programming。凸优化问题是指约束最优化问题
$$
\min_w\; f(w)\\
s.t.\; g_i(w)\le0,\;i=1,2,···,k\\
h_i(w)=0,\:i=1,2,···,k
$$
其中，目标函数$f(w)$和约束函数$g_i(w)$都是连续可微的凸函数。

**线性可分训练数据集的最大间隔分离超平面存在的唯一性**

（待补充）

在线性可分情况下，训练数据集的样本中，于分离超平面距离最近的样本点的实例称为支持向量，support vector。支持向量使得约束条件成立。即
$$
y_i(w\cdot x_i+b)-1=0
$$
对于$y_i=+1$的正例点，支持向量在超平面H1：$w\cdot x_i+b=1$上。负例点在H2：$w\cdot x_i+b=-1$上。注意到H1与H2平行，且无实例点落在他们中间，且分离超平面在他们的中央且平行于他们。H1与H2之间的距离称为间隔，margin。间隔依赖于分离超平面的法向量，为$\frac{2}{||w||}$

可以看到，在**决定分离超平面时，只有支持向量其作用**，支持向量的个数一般很少，所以SVM由很少的训练样本确定。

#### 7.1.4 学习的对偶算法

为了求解线性可分支持向量机的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题，dual problem，得到原始问题，primal problem的最优解。定义拉格朗日函数如下
$$
L(w,b,a)=\frac{1}{2}||w||^2-\sum_{i=1}^Na_iy_i(w\cdot x_i+b)+\sum_{i=1}^Na_i
$$
其中$a_i\ge0,\;i=1,2,···,N$为拉格朗日乘子，Lagrange multiplier。

原始问题的对偶问题是极大极小问题：
$$
\max_a\min_{w,b}L(w,b,a)
$$
先求min问题，得
$$
\nabla_wL(w,b,a)=w-\sum_{i=1}^Na_iy_ix_i=0\\
\nabla_bL(w,b,a)=\sum_{i=1}^Na_iy_i=0
$$
将上式代入L中得
$$
\begin{align*}
\min_{w,b}L(w,b,a)&=\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Na_ia_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^Na_iy_i((\sum_{i=1}^Na_iy_ix_i)\cdot x_i+b)+\sum_{i=1}^Na_i\\
&=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Na_ia_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^Na_i
\end{align*}
$$
在求上式最大值，有对偶问题转变为如下形式
$$
\max_a\;-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Na_ia_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^Na_i\\
s.t.\;\;\sum_{i=1}^Na_iy_i=0\\
a_i\ge0
$$

### 7.2 线性支持向量机与软间隔最大化

