# 统计学习方法 （李航）

[toc]

## 第七章 支持向量机

### 7.1 线性可分支持向量机与硬间隔最大化

支持向量机的学习是在特征空间进行的。

#### 7.1.1 线性可分支持向量机

考虑一个二分问题，学习的目的是在特征空间找到一个分离超平面，能将实例分到不同过的类。分离超平面对应于方程$w\cdot x+b=0$，它由法向量w和截距b决定。

一般的，当训练数据集线性可分是，存在无穷个分离超平面，类比于上下确界。感知机利用误分类最小的策略，求得分离超平面，从而解有无穷多个。线性可分支持向量机利用间隔最大化求最有分离超平面，这时，**解释唯一的**。

定义

线性可分支持向量机，给定线性可分训练数据集，通过间隔最大化，或等价地求解相应的凸二次规划问题学习得到的分离超平面为
$$
w^{*}\cdot x+b^*=0
$$
相应的分类决策函数为
$$
f(x)=sign(w^{*}\cdot x+b^*)
$$
称为线性可分支持向量机

#### 7.1.2 函数间隔和几何间隔

一般来说，**一个点距离分离超平面的远近，可以表示分类预测的确信程度。**在超平面确定的情况下，$|w\cdot x+b|$能相对地表示点距离超平面的远近，而$w\cdot x+b$的符号于类标记y的符号是否一致，表示分类是否正确。

所以$y(w\cdot x+b)$表示分类的正确性以及确信度。这就是函数间隔的概念，functional margin。

但是只要成比例的改变w和b，例如将它们改为2w和2b，超平面并没有改变，但是函数间隔变为了原来的2倍。

所以对分离超平面的法向量加以约束，如规范化，$||w||=1$，使得间隔是确定的。这时函数间隔变为几何间隔，geometric margin。
$$
\gamma_i=y_i(\frac{w}{||w||}\cdot x_i+\frac{b}{||w||})
$$

#### 7.1.3 间隔最大化

间隔最大化的直观解释是，对训练数据集找到集合间隔最大的超平面意味着以充分大的确信度，对训练数据进行分类。这里的间隔最大化又称为硬间隔最大化。

考虑下面约束优化问题
$$
\max_{w,b}\;\gamma\\
s.t.\; y_i(\frac{w}{||w||}\cdot x_i+\frac{b}{||w||})\ge\gamma,\;i=1,2,···,N
$$
通过设定$\gamma\ge0$来使得分类正确
$$
\max_{w,b}\;\frac{\hat{\gamma}}{||w||}\\
s.t.\; y_i(w\cdot x_i+b)\ge\hat\gamma,\;i=1,2,···,N
$$
其中$\hat\gamma=\min_iy_i(w\cdot x_i+b)$,是函数间隔。**由于函数间隔的大小并不影响最优化问题的解，事实上，假设将w和b按比例改变为$\lambda w,\;\lambda b$，这时函数间隔称为$\lambda\hat\gamma$。**函数间隔的这一改变，对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响。即，产生一个等价的最优化问题。

这样，我们取$\hat\gamma=1$，将其带入上述的最优化问题。同时又注意到，最大化$\frac{1}{||w||}$同最小化$\frac{||w||^2}{2}$等价，从而有以下最优化问题
$$
\min_{w,b}\frac{||w||^2}{2}\\
s.t.\; y_i(w\cdot x_i+b)-1\ge0,\;i=1,2,···,N
$$

这是一个凸二次规划，convex quadratic programming。凸优化问题是指约束最优化问题
$$
\min_w\; f(w)\\
s.t.\; g_i(w)\le0,\;i=1,2,···,k\\
h_i(w)=0,\:i=1,2,···,k
$$
其中，目标函数$f(w)$和约束函数$g_i(w)$都是连续可微的凸函数。

**线性可分训练数据集的最大间隔分离超平面存在的唯一性**

（待补充）

在线性可分情况下，训练数据集的样本中，于分离超平面距离最近的样本点的实例称为支持向量，support vector。支持向量使得约束条件成立。即
$$
y_i(w\cdot x_i+b)-1=0
$$
对于$y_i=+1$的正例点，支持向量在超平面H1：$w\cdot x_i+b=1$上。负例点在H2：$w\cdot x_i+b=-1$上。注意到H1与H2平行，且无实例点落在他们中间，且分离超平面在他们的中央且平行于他们。H1与H2之间的距离称为间隔，margin。间隔依赖于分离超平面的法向量，为$\frac{2}{||w||}$

可以看到，在**决定分离超平面时，只有支持向量其作用**，支持向量的个数一般很少，所以SVM由很少的训练样本确定。

#### 7.1.4 学习的对偶算法

为了求解线性可分支持向量机的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题，dual problem，得到原始问题，primal problem的最优解。定义拉格朗日函数如下
$$
L(w,b,a)=\frac{1}{2}||w||^2-\sum_{i=1}^Na_iy_i(w\cdot x_i+b)+\sum_{i=1}^Na_i
$$
其中$a_i\ge0,\;i=1,2,···,N$为拉格朗日乘子，Lagrange multiplier。

原始问题的对偶问题是极大极小问题：
$$
\max_a\min_{w,b}L(w,b,a)
$$
先求min问题，得
$$
\nabla_wL(w,b,a)=w-\sum_{i=1}^Na_iy_ix_i=0\\
\nabla_bL(w,b,a)=\sum_{i=1}^Na_iy_i=0
$$
将上式代入L中得
$$
\begin{align*}
\min_{w,b}L(w,b,a)&=\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Na_ia_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^Na_iy_i((\sum_{i=1}^Na_iy_ix_i)\cdot x_i+b)+\sum_{i=1}^Na_i\\
&=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Na_ia_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^Na_i
\end{align*}
$$
在求上式最大值，有对偶问题转变为如下形式
$$
\max_a\;-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Na_ia_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^Na_i\\
s.t.\;\;\sum_{i=1}^Na_iy_i=0\\
a_i\ge0
$$
定理 设$a^*=(a_1^*,a_2^*,···,a_l^*)^T$是对偶问题的解，则存在下标j，使得$a_j^*>0$，并可按下式求得原始最优化问题的解
$$
w^*=\sum_{i=1}^Na_i^*y_ix_i\\
b^*=y_j-\sum_{i=1}^Na_i^*y_i(x_i\cdot x_j)
$$


### 7.2 线性支持向量机与软间隔最大化

#### 7.2.1 线性支持向量机

对线性不可分训练集，不等式约束条件不都能成立，即**某些样本点，不能满足函数间隔大于等于1**。

为了解决这个问题，可以每个样本点引进一个松弛变量$\xi_i\ge0$，使得函数间隔加上松弛变量大于等于1，即
$$
y_i(w\cdot x_i+b)\ge1-\xi_i
$$
同时对每个松弛变量，支付相应代价，使得目标函数变为
$$
\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i
$$
$C>0$称为惩罚参数。

则原始问题为如下形式
$$
\min_{w,b}\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i\\
s.t.\; y_i(w\cdot x_i+b)\ge1-\xi_i,\;i=1,2,···,N\\
\xi_i\ge0,\;i=1,2,···,N
$$
相应得对偶问题，可推到如下。首先写出拉格朗日方程
$$
L(w,b,\xi,a,\mu)=\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i-\sum_{i=1}^Na_i(y_i(w\cdot x_i+b)-1+\xi_i)-\sum_{i=1}^N\mu_i\xi_i
$$
求gradient有
$$
\nabla_wL(w,b,\xi,a,\mu)=w-\sum_{i=1}^Na_iy_ix_i=0\\
\nabla_bL(w,b,\xi,a,\mu)=-\sum_{i=1}^Na_iy_i=0\\
\nabla_{\xi_i}L(w,b,\xi,a,\mu)=C-a_i-\xi_i=0
$$
将上述约束带入得
$$
\min_{w,b,\xi}L(w,b,\xi,a,\mu)=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Na_ia_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^Na_i
$$
再求对a得极大，即为对偶问题

#### 7.2.4 合页损失函数

线性支持向量机学习还有另外一种解释，就是最小化以下目标函数
$$
\sum_{i=1}^N[1-y_i(w\cdot x_i+b)]_++\lambda||w||^2
$$
目标函数的第一项是经验损失或经验风险函数
$$
L(y(w\cdot x+b))=[1-y(w\cdot x+b)]_+
$$
称为合页损失函数，hinge loss function
$$
[z]_+=\begin{cases}
z, & z>0\\
0, & z\le0
\end{cases}
$$
即当样本点被正确分类且函数间隔，确信度大于1时，损失函数时0.否则为$1-y_i(w\cdot x_i +b)$.

定理 先行支持向量机原始优化问题等价于最优化问题
$$
\min_{w,b}\sum_{i=1}^N[1-y_i(w\cdot x_i+b)]_++\lambda||w||^2
$$

### 7.3 非线性支持向量机与和函数

#### 7.3.1 核技巧

1. 非线性分类

一般来说，对给定的一个训练数据集$T=\{(x_1,y_1),(x_2,y_2),···，(x_N,y_N)\}$，其中$x_i\in\mathcal{X}=\mathbf{R}^n$，对应的标记有两类$y_i\in\mathcal{Y}=\{-1,+1\}$。如果能用$\mathbf{R}^n$中的一个超曲面将正负例分开，则称这个问题为非线性可分问题。

2. 核函数的定义

设$\mathcal{X}$是输入空间，一般为欧式空间$\mathbf{R}^n$的子集或离散集合，设$\mathcal{H}$为特征空间，如果存在一个$\mathcal{X}$到$\mathcal{H}$的映射
$$
\phi(x):\mathcal{X}\rightarrow \mathcal{H}
$$
使得对所有$x,z\in\mathcal{X},K(x,z)$函数有
$$
K(x,z)=\phi(x)\phi(z)
$$
上式为$\phi(x)$与$\phi(z)$的内积。

