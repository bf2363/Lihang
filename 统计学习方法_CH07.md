# 统计学习方法 （李航）

[toc]

## 第七章 支持向量机

### 7.1 线性可分支持向量机与硬间隔最大化

支持向量机的学习是在特征空间进行的。

#### 7.1.1 线性可分支持向量机

考虑一个二分问题，学习的目的是在特征空间找到一个分离超平面，能将实例分到不同过的类。分离超平面对应于方程$w\cdot x+b=0$，它由法向量w和截距b决定。

一般的，当训练数据集线性可分是，存在无穷个分离超平面，类比于上下确界。感知机利用误分类最小的策略，求得分离超平面，从而解有无穷多个。线性可分支持向量机利用间隔最大化求最有分离超平面，这时，**解释唯一的**。

定义

线性可分支持向量机，给定线性可分训练数据集，通过间隔最大化，或等价地求解相应的凸二次规划问题学习得到的分离超平面为
$$
w^{*}\cdot x+b^*=0
$$
相应的分类决策函数为
$$
f(x)=sign(w^{*}\cdot x+b^*)
$$
称为线性可分支持向量机

#### 7.1.2 函数间隔和几何间隔

一般来说，**一个点距离分离超平面的远近，可以表示分类预测的确信程度。**在超平面确定的情况下，$|w\cdot x+b|$能相对地表示点距离超平面的远近，而$w\cdot x+b$的符号于类标记y的符号是否一致，表示分类是否正确。

所以$y(w\cdot x+b)$表示分类的正确性以及确信度。这就是函数间隔的概念，functional margin。

但是只要成比例的改变w和b，例如将它们改为2w和2b，超平面并没有改变，但是函数间隔变为了原来的2倍。

所以对分离超平面的法向量加以约束，如规范化，$||w||=1$，使得间隔是确定的。这时函数间隔变为几何间隔，geometric margin。
$$
\gamma_i=y_i(\frac{w}{||w||}\cdot x_i+\frac{b}{||w||})
$$

#### 7.1.3 间隔最大化

间隔最大化的直观解释是，对训练数据集找到集合间隔最大的超平面意味着以充分大的确信度，对训练数据进行分类。这里的间隔最大化又称为硬间隔最大化。

考虑下面约束优化问题
$$
\max_{w,b}\;\gamma\\
s.t.\; y_i(\frac{w}{||w||}\cdot x_i+\frac{b}{||w||})\ge\gamma,\;i=1,2,···,N
$$
通过设定$\gamma\ge0$来使得分类正确
$$
\max_{w,b}\;\frac{\hat{\gamma}}{||w||}\\
s.t.\; y_i(w\cdot x_i+b)\ge\hat\gamma,\;i=1,2,···,N
$$
其中$\hat\gamma=\min_iy_i(w\cdot x_i+b)$,是函数间隔。**由于函数间隔的大小并不影响最优化问题的解，事实上，假设将w和b按比例改变为$\lambda w,\;\lambda b$，这时函数间隔称为$\lambda\hat\gamma$。**函数间隔的这一改变，对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响。即，产生一个等价的最优化问题。

这样，我们取$\hat\gamma=1$，将其带入上述的最优化问题。同时又注意到，最大化$\frac{1}{||w||}$同最小化$\frac{||w||^2}{2}$等价，从而有以下最优化问题
$$
\min_{w,b}\frac{||w||^2}{2}\\
s.t.\; y_i(w\cdot x_i+b)-1\ge0,\;i=1,2,···,N
$$
