# 统计学习方法 （李航）

[toc]

## 第五章 决策树

### 5.1 决策树模型与学习

#### 5.1.1 决策树模型

定义，分类决策树模型是一种描述对实例进行分类的属性结构。决策树由**结点，node和有向边，directed edge组成**。结点有两种类型：内部结点，internal node，和叶结点，leaf node。**内部结点表示一个特征或属性，叶结点表示一个类。**

#### 5.1.2 决策树与if-then规则

#### 5.1.3 决策树与条件概率分布

决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分，partition。将特征区间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。**决策树的一条路径对应于划分中的一个单元。**

#### 5.1.4 决策树学习

决策树学习**本质上是从训练数据集中归纳出一组分类规则。**与训练数据集不相矛盾的决策树，**即能对训练数据进行正确分类的决策树**可能有多个，也可能没有。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。从另一个角度看，决策树学习是由训练数据集估计条件概率模型，基于特征空间划分的类的条件概率模型由无穷多个，我们选择的条件概率模型应该不仅对训练数据有好的你和，而且对未知数据有好的预测。

决策树学习用**损失函数**表示这一目标。决策树学习的**损失函数通常是正则化的极大似然函数**。决策树学习的策略是以损失函数为目标函数的最小化。

当损失函数确定后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为**从所有可能的决策树中选取最优决策树是NP完全问题**，所以现实中决策树学习算法通常采用**启发式方法**，近似求解这一最优化问题。这样得到的决策树是**次最优，sub-optimal。**

### 5.2 特征选择

#### 5.2.1 特征选择问题

特征选择在于选取对悬链数据具有分类能力的特征。通常**特征选择的准则是信息增益或信息增益比**。

#### 5.2.2 信息增益

熵，entropy是表示随机变量不确定性的度量，设X是一个取有限个值的离散随机变量，其概率分布为
$$
P(X=x_i)=p_i
$$
则随机变量X的熵定义为
$$
H(X) = -\sum_{i=1}^np_i\log p_i
$$
熵越大，随机变量的不确定性就越大，即均匀分布。

设随机变量$(X,Y)$，其联合概率分布为
$$
P(X=x_i,Y=y_j)=p_{ij}
$$
条件熵$H(Y|X)$表示在已知随机变量X的条件下随机变量Y的不确定性，定义为X给定条件下Y的条件概率分布的熵对X的数学期望
$$
H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)
$$
当熵和条件熵的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为**经验熵和经验条件熵**，empirical entropy，empirical conditional entropy。

信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。

定义，特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即为
$$
g(D,A)=H(D)-H(D|A)
$$
设训练集数据集为D，|D|表示其容量，设K个类$C_k，|C_k|$表示属于该类的样本个数。$\sum_{k=1}^K|C_k|=|D|$，设特征A由n个不同的取值$\{a_1,a_2,···,a_n\}$，根据特征A的取值，将D划分为n个子集，$D_i$，记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$，即$D_{ik}=D_i\bigcap C_k$。

信息增益算法如下，

（1）计算数据集D的经验熵
$$
H(D) = -\sum_{k=1}^K\frac{|C_k|}{|D|}\log\frac{|C_k|}{|D|}
$$
（2）计算特征A对数据集的经验条件熵
$$
H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log\frac{|D_{ik}|}{|D_i|}
$$
（3）计算信息增益
$$
g(D,A)=H(D)-H(D|A)
$$

#### 5.2.3 信息增益比

以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比，information gain ratio可以对这一问题进行校正。

定义
$$
g_r(D,A)=\frac{g(D,A)}{H_A(D)}
$$
其中，$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log \frac{|D_i|}{|D|}$，n为特征A取值的个数

### 5.3 决策树的生成

#### 5.3.1 ID3算法

（1）若D中所有实例属于同一类$C_k$，则T为单结点树，并将类$C_k$作为该结点的类标记，返回T；

（2）若$A=\empty$，则T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T；

（3）否则，计算A中各特征对D的信息增益，选择信息增益最大的特征$A_g$；

（4）如果$A_g$的信息增益小于阈值$\epsilon$，则T为单结点树，并将D中实例树最大的类$C_k$作为该结点的类标记，返回T；

（5）否则对$A_g$的每一个可能值$a_i$，依$A_g=a_i$，将D分给为若干非空子集$D_i$，将$D_i$中实例树最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T；

（6）对第i个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归地调用前五步，都得到子树$T_i$，返回$T_i$。

**ID3算法只有树的生成，所以容易产生过拟合**。

#### 5.3.2 C4.5的生成算法

相较于ID3，C4.5使用了信息增益比来选择特征。

### 5.4 决策树的剪枝

决策树的剪枝往往通过极小化决策树整体的损失函数，loss function，或代价函数cost function来实现。设树T的叶结点个数为|T|，t是树T的叶结点，该叶结点由$N_t$个样本点，其中k类的样本点由$N_{tk}$个，$H_t(T)$为该点t上的经验熵，$\alpha\ge0$为参数，则决策树学习的损失函数可以定位为
$$
C_{\alpha}(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|
$$
其中经验熵为
$$
H_t(T)=-\sum_k\frac{|N_{tk}|}{|N_t|}\log \frac{|N_{tk}|}{|N_t|}
$$
树的剪枝算法

（1）计算每个结点的经验熵

（2）递归地从树的叶结点向上回缩。

设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_B$和$T_A$，与其对应的损失函数分别$C_\alpha(T_B)$与$C_\alpha(T_A)$，如果
$$
C_\alpha(T_A)\le C_\alpha(T_B)
$$
则进行剪枝，即父结点变为新的叶结点。

（3）重复2，直至不能继续为止，得到损失函数最小的子树。

### 5.5 CART算法

CART是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。CART假设决策树是二分树，内部结点特征的取值为**是和否**，左分支取“是”，右分支取“否”。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是输入给定的条件下输出条件概率分布。

#### 5.5.1 CART生成

1）回归树的生成

给定训练数据集
$$
D=\{(x_1,y_1),(x_2,y_2)···,(x_N,y_N)\}
$$
其中Y是连续变量。

一个回归树对应着输入空间的一个划分以及在划分的单元的输出值。假设已将输入空间划分为M的单元$R_1,R_2,···,R_M$，并且在每个单元上有一个固定的输出值$c_m$，于是回归树模型可表示为
$$
f(x)=\sum_{m=1}^Mc_mI(x\in R_m)
$$
当输入空间的划分确定时，可以用平方误差$\sum_{x_i\in R_m}(y_i-f(x_i))^2$来表示回归树的预测误差。易知，单元上的$c_m$的最优值$\hat{c}_m$是单元上所有实例$x_i$对应的输出的均值。

对于怎样对输入空间进行划分，这里采用**启发式的方法**，选择第J个变量$x^{j}$和他的取值s，作为切分变量，splitting variable和切分点，并定义两个区域：
$$
R_1(j,s)=\{x|x^{(j)}\le s\},R_2(j,s)=\{x|x^{(j)}> s\}
$$
然后寻找最优切分变量j和最优切分点s，即求解
$$
\min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]
$$
算法

（1） 选择最优切分变量j与切分点s，求解
$$
\min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]
$$
遍历变量j，对古典的切分变量j，扫描切分点s，选择使得上式达到最小值的对（j，s）

（2）用选定的对（j，s）划分区域并决定相应的输出值：
$$
R_1(j,s)=\{x|x^{(j)}\le s\},R_2(j,s)=\{x|x^{(j)}> s\}\\
\hat{c}_m=\frac{1}{N_m}\sum_{x_i\in R_m(j,s)}y_i, x\in R_m,m=1,2
$$
（3）继续对两个子区域调用上述步骤，直至满足停止条件

（4）将输入空间划分为M个区域，生成决策树：
$$
f(x)=\sum_{m=1}^M\hat{c}_mI(x\in R_m)
$$
2）分类树的生成

分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。

基尼指数定义为，分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼指数定义为
$$
Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1 - \sum_{k=1}^Kp_k^2
$$
如果样本集合D根据特征A是否取某一可能值$\alpha$被分割为$D_1$和$D_2$两部分，则在特征A的条件下，集合D的基尼指数定义为
$$
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
$$
基尼指数表示集合D的不确定性，指数越大，样本的不确定性也就越大。

算法

（1）设节点的训练集为D，计算现有特征对该数据集的基尼指数，此时，对每一个特征A，对其可能取得每个是a，根据样本点对A=a的测试为“是”或“否”将D分割成D1和D2两部分。利用式(21)，计算A=a的基尼指数。

（2）在所有可能的特征以及他们可能的切分点a中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依据最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中取。

（3）对两个子结点递归的调用（1）（2）直至满足停止条件。

#### 5.5.2 CART剪枝

CART剪枝算法由两步组成：首先从算法产生的决策树底端开始不断剪枝，直到根结点，形成一个子树序列$\{T_0,T_1,···,T_n\}$；然后通过交叉验证在独立的验证数据上对子树序列进行测试，选择最优的子树。

在剪枝过程中，计算子树的损失函数，形式为，
$$
C_\alpha(T)=C(T)+\alpha|T|
$$
其中$C(T)$为对训练数据的预测误差，如基尼指数。

1）剪枝，形成一个子树序列

可以证明：可以用递归的方法对树进行剪枝。将a从小增大,
$$
0=\alpha_0<\alpha_1<···<\alpha_n<+\infty
$$
产生一系列的区间$[\alpha_i,\alpha_{i+1})$，剪枝得到的子树对饮跟着相应区间。

具体的，从整体树$T_0$开始剪枝。对其任意内部结点t,以t为单节点树的损失函数为
$$
C_\alpha(t)=C(t)+\alpha
$$
以t为根节点的子树$T_t$的损失函数是
$$
C_\alpha(T_t)=C(T_t)+\alpha|T_t|
$$
当$\alpha=0$以及充分小的时候，有不等式
$$
C_\alpha(T_t) < C_\alpha(T)
$$
当$\alpha=\frac{C(t)-C(T_t)}{|T_t|-1}$的时候，有二者相等，当$\alpha$在增大时，将反向。而当二者相等时，$T_t$与t有相同的损失函数，而t的结点少，因此t比$T_t$更可取，所以对$T_t$进行剪枝。

为此，对T0每个内部结点t，计算
$$
g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}
$$
其表示剪枝后整体损失函数减少的程度。在T0中减去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$\alpha_1$。 $T_1为区间[\alpha_1,\alpha_2)$的最优子树。

2）在剪枝得到子树序列后，通过交叉验证选取最优子树。